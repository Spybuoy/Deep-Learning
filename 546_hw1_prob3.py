# -*- coding: utf-8 -*-
"""546_HW1_prob3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15lvTpY2BXq_8QgfPuJ5MACvh-4Ubi5-F
"""

"""
importing files
"""
import numpy as np
import random
import matplotlib.pyplot as plt
import torch
import torchvision as thv
from scipy.special import y1
import torchvision as thv
from torch.utils.data import DataLoader
from torchvision.transforms import ToTensor
from torch import nn
import torch.nn.functional as F

"""
Downloading the datasets-3a
"""
train = thv.datasets.MNIST('./', download=True, train=True)
val = thv.datasets.MNIST('./', download=True, train=False)
"""
Checking if we have correct images-3a
"""
def show_images(x_train,y_train):
  r1 = list(np.arange(30000))
  idx=random.sample(r1,5)
  fig, row = plt.subplots(1,5,figsize=(16, 16))
  for i in range(len(idx)):
      xlabel = f'label={y_train[idx[i]]}'
      row[i].set_title('Training data'+str(i))
      row[i].set_xlabel(xlabel)
      row[i].imshow(x_train[idx[i]].reshape(28,28))
  r2 = list(np.arange(5000))
  idx2=random.sample(r2,5)
  fig2, row2 = plt.subplots(1,5,figsize=(16, 16))
  for i in range(len(idx2)):
      xlabel = f'label={y_val[idx2[i]]}'
      row2[i].set_title('Validataion data'+str(i))
      row2[i].set_xlabel(xlabel)
      row2[i].imshow(x_val[idx2[i]].reshape(28,28))
"""
splitting data into train and val
"""
def data():
  x_train=np.zeros([30000,28,28])
  x_val=np.zeros([5000,28,28])
  y_train=np.zeros([30000,])
  y_val=np.zeros([5000,])
  c1,c2 = 0,0

  for i in range(0,10):
    lst=np.where(train.targets==i)[0]
    lst = list(lst)
    random_idx=random.sample(lst,3000)
    x_train[c1:c1+3000]=train.data[random_idx]
    y_train[c1:c1+3000]=train.targets[random_idx]
    c1 = c1 + 3000
    lst2=np.where(val.targets==i)[0]
    lst2 = list(lst2)
    random_idx2=random.sample(lst2,500)
    x_val[c2:c2+500]=val.data[random_idx2]
    y_val[c2:c2+500]=val.targets[random_idx2]
    c2 = c2 + 500
  l=np.arange(0,30000)
  np.random.shuffle(l)
  x_train=x_train[l].reshape(-1,784)/255
  y_train=np.array(y_train[l])
  l2=np.arange(0,5000)
  np.random.shuffle(l2)
  x_val=x_val[l2].reshape(-1,784)/255
  y_val=np.array(y_val[l2])
  return x_train,y_train,x_val,y_val

"""
Creating layers of nerual network-3b
"""
class linear_t:
  def __init__(self):
    # initialize to appropriate sizes, fill with Gaussian entires
    # normalize to make the Frobenius norm of w, b equal to 1
    """
    Do not use pytorch now
    """
    # self.w, self.b = torch.randn(10,784),torch.randn(1,10)
    self.w, self.b = np.random.randn(10,784),np.random.randn(1,10)
    self.w,self.b=self.w/np.linalg.norm(self.w, 'fro'),self.b/np.linalg.norm(self.b, 'fro')
  def forward(self, hi):
    #hi=1x784
    ho = (hi@self.w.T)+self.b
    # cache h^l in forward because we will need it to compute
    # dw in backward
    self.hi = hi
    return ho
  def backward(self, dho):
    dhi= (dho@(self.w))
    # self.hi=np.zeros(dhi.shape)
    dw, db = (dho.T)@(self.hi)/dho.shape[0],np.mean(dho,axis=0)
    self.dw, self.db = dw, db
    # notice that there is no need to cache dh^l
    return dhi
  def zero_grad(self):
    # useful to delete the stored backprop gradients of the
    # previous mini-batch before you start a new mini-batch
    self.dw, self.db = 0*self.dw, 0*self.db

"""
Relu-3c
"""
class relu_t(object):
  def forward(self,hi):
    self.ho=np.maximum(0,hi)
    self.hi=hi
    return self.ho
  def backward(self,dho):
    a=self.hi>0
    dhi=dho*a
    return dhi
"""
Softmax and cross entropy layer -3d
"""
class softmax_cross_entropy_t(object):
    def forward(self, hi, y):
        deno=np.sum(np.exp(hi), axis=1)
        deno=deno.reshape(deno.shape[0],1)
        ho = np.exp(hi) / deno
        a=np.zeros([y.shape[0],10])
        for i in range(y.shape[0]):
          b=y_train[i]
          a[i,int(y[i])]=1
        loss = (np.log(ho) * a).sum(axis=1)
        ell = -np.mean(loss)
        error = np.sum(y != np.argmax(ho, axis=1))
        error*=1 / len(y)  
        self.y = y
        self.ho=ho
        self.a=a
        return ell, error
    def backward(self):
        temp = self.ho - self.a
        return temp
"""
Gradient check - 3e
"""
def gradient_backtrack():
  for j in range(10):
    for i in range(5):
      """
      Creating matrices
      """
      e=np.zeros([10,784])
      e[j,i]=10^(-4)
      hi=np.random.normal(size=(1,784))
      dho=np.zeros([1,10])
      dho[0,i]=1
      e_hi=np.zeros([1,784])
      e_hi[0,i]=10^(-4)
      w_hi=np.random.normal(size=(10,784))
      e_b=np.zeros([10,1])
      e_b[i,0]=10^(-4)
      """
      Calling linear class
      """
      linear=linear_t()
      ho=linear.forward(hi)
      dhi=linear.backward(dho)
      w=linear.w
      dw=linear.dw
      hi=linear.hi
      db=linear.db
      """
      Checking for error
      """
      dw_ew = ((hi@(w+e).T)[0,i]-(hi@(w-e).T)[0,i])/(2*e[j,i])
      dw_ehi=((hi+e_hi)@w.T-(hi-e_hi)@w.T)/((2*e_hi[0,i]))
      db_eb=(db+e_b-(db-e_b))/((2*e_b[i,0]))
      print('error for W = ',dw_ew-dw[0,i])
      print('error for hi = ',dhi[0,i]-dw_ehi[0,i])
      print('error for b = ',np.sum(db[i]-db_eb[i]))
      print("--------------------------------------")
"""
Validation error calculation - 3g
"""
def validate(w, b):
    ell, error = 0, 0
    for i in range(0, 1000, 32):
        x, y = x_val[i:i+32], y_val[i:i+32]
        hi = (x@w.T+b)
        ho = relu_t().forward(hi)
        deno=(np.sum(np.exp(ho), axis=1))
        deno=deno.reshape(deno.shape[0],1)
        a=np.zeros([y.shape[0],10])
        for i in range(y.shape[0]):
          b=y_train[i]
          a[i,int(y[i])]=1
        ell = (np.log(np.exp(ho) / deno) * a).sum(axis=1)
        ell = -np.mean(ell)
        error += np.sum(y != np.argmax(ho, axis=1))/1000
    return ell, error

x_train,y_train,x_val,y_val=data()
show_images(x_train,y_train)

"""
Calling the different layers-3f
"""
l1,l2,l3 = linear_t(), relu_t(), softmax_cross_entropy_t()
net = [l1,l2,l3]
"""
Creating Matrices
"""
train_loss = np.zeros((15000,1))
train_error=np.zeros((15000,1))

val_loss=[]
val_error=[]

for t in range(15000):
    a = np.random.choice(x_train.shape[0], size =200)
    x, y = x_train[a], y_train[a]

    for l in net:
        try:
            l.zero_grad()
        except:pass
        
    """
    foward pass
    """
    h1 = l1.forward(x)
    h2 = l2.forward(h1)
    ell, error = l3.forward(h2, y)

    """
    backward pass
    """
    dh2 = l3.backward()
    dh1 = l2.backward(dh2)
    dx = l1.backward(dh1)

    """
    gather backdrop gradients
    """
    dw, db = l1.dw, l1.db

    """
    printing quantities
    """
    print(t, ell, error)

    
    train_loss[t ] = ell
    train_error[t ] = error
    
    ell1,error1=validate(l1.w,l1.b)
    val_loss.append(ell1)
    val_error.append(error1)

    """
    SGD 1 step
    """
    l1.w = l1.w - 0.01* dw
    l1.b = l1.b - 0.01* db

    
"""
plotting images
"""
fig, row = plt.subplots(2,1,figsize=(8,10))
row[0].plot(range(15000), train_loss, color = 'red')
row[0].set_xlabel('iterations')
row[0].set_ylabel('Traininig loss')
row[0].set_title('Training loss vs iterations')
row[1].plot(range(15000), train_error, color = 'Blue')
plt.legend(loc = 'best')
row[1].set_xlabel('iterations')
row[1].set_ylabel('Traininig error')
row[1].set_title('Training error vs iterations')

fig, row1 = plt.subplots(2,1,figsize=(8,10))
row1[0].plot(range(15000), val_loss, color = 'green')
row1[0].set_xlabel('iterations')
row1[0].set_ylabel('Validataion loss')
row1[0].set_title('Validation loss vs iterations')
row1[1].plot(range(15000), val_error, color = 'brown')
row1[1].set_xlabel('iterations')
row1[1].set_ylabel('Validation error')
row1[1].set_title('Validation error vs iterations')

gradient_backtrack()

# initializing the datasets
"""
Pytorch - Insipration from ESE539 pytorch tutorial
"""
train = thv.datasets.MNIST('./', download=True, train=True, transform=ToTensor())
val = thv.datasets.MNIST('./', download=True, train=False, transform=ToTensor())

tr = DataLoader(train, batch_size=150)
te = DataLoader(val, batch_size=150)

class NN(nn.Module):
    def __init__(self):
        super().__init__()
        self.l1 = nn.Linear(784, 10)
        self.flatten = nn.Flatten()
    def forward(self, x):
        return F.relu(self.l1(self.flatten(x)))


model = NN()
loss = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum = 0.9)
(x,y,x1,y1)=data()

# def show(ip):
#   x = [1,2,3,4,5]
#   y = ip
#   plt.bar(x,y)
#   plt.xlabel('no of epochs')
#   plt.ylabel('error')
#   plt.ylim(10,20)
#   plt.show()

for t in range(5):
    print(f"Epoch {t+1}")
    model.train()
    err = []
    for (x, y) in tr:
        l = loss(model(x), y)
        optimizer.zero_grad()
        l.backward()
        optimizer.step()
        # print(l)
    model.eval()
    total=0
    correct=0 
    for x, y in te:   
      r = model(x.view(-1, 784))
      for index, i in enumerate(r):
          if torch.argmax(i) == y[index]:
                    correct = correct + 1
          total = total + 1
          acc = 100*correct/total
    print('Accuracy '+str (acc)+'%')
    """
    top-N accuracy can be used where N==1, we get the same accuracy result.
    """
    print('Error '+str (100-acc)+'%')
    err.append(100-acc)
    print('-----------------')
print("Done!")
print("*************")
# show(err)